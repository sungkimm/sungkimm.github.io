{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"#_1","text":"","title":""},{"location":"Projects/","text":"Project List gempick","title":"Project List"},{"location":"Projects/#project-list","text":"gempick","title":"Project List"},{"location":"Projects/gempick/dev-materials/OAuth/","text":"OAuth 2.0 Links [Youtube] Google OAuth 2.0 with NodeJS OAuth2.0 official [Youtube] OAuth 2.0 and OpenID Connect OAuth2.0 \uac1c\ub150\uc815\ub9ac","title":"OAuth 2.0"},{"location":"Projects/gempick/dev-materials/OAuth/#oauth-20","text":"","title":"OAuth 2.0"},{"location":"Projects/gempick/dev-materials/OAuth/#links","text":"[Youtube] Google OAuth 2.0 with NodeJS OAuth2.0 official [Youtube] OAuth 2.0 and OpenID Connect OAuth2.0 \uac1c\ub150\uc815\ub9ac","title":"Links"},{"location":"Projects/gempick/dev-materials/aws/","text":"","title":"Aws"},{"location":"Projects/gempick/dev-materials/git/","text":"","title":"Git"},{"location":"Projects/gempick/dev-materials/restAPI/","text":"RESTAPI FastAPI Best Practice Best Practice 1 Best Practice 2 RestAPI Security Best Practice Security Cheat Sheet","title":"RESTAPI"},{"location":"Projects/gempick/dev-materials/restAPI/#restapi","text":"","title":"RESTAPI"},{"location":"Projects/gempick/dev-materials/restAPI/#fastapi-best-practice","text":"Best Practice 1 Best Practice 2","title":"FastAPI Best Practice"},{"location":"Projects/gempick/dev-materials/restAPI/#restapi-security-best-practice","text":"Security Cheat Sheet","title":"RestAPI Security Best Practice"},{"location":"Projects/gempick/dev-materials/microservice%28k8s%29/microservice%28k8s%29/","text":"MicroService(k8s) MicroService in General 10 Principles [Youtube-nana] MSA Overview [Youtube] Design Microservice Architectures the Right Way MicroService Design Patterns [Youtube] Microservices Architecture Design Patterns | 10 Design Principles | 26 Design Patterns Circuit Breaker Pattern [Youtube] Circuit Breaker Pattern API Gateway Links [Youtube] API Gateway API Gateway pattern Istio & Service Mesh [Youtube] Istio Setup in Kubernetes | Step by Step Guide to install Istio Service Mesh [Youtube] Istio & Service Mesh - simply explained in 15 mins [Youtube] What is a service mesh? [Youtube] Istio Service Istio Challenge of MicroService Archeitecture","title":"MicroService(k8s)"},{"location":"Projects/gempick/dev-materials/microservice%28k8s%29/microservice%28k8s%29/#microservicek8s","text":"","title":"MicroService(k8s)"},{"location":"Projects/gempick/dev-materials/microservice%28k8s%29/microservice%28k8s%29/#microservice-in-general","text":"10 Principles [Youtube-nana] MSA Overview [Youtube] Design Microservice Architectures the Right Way","title":"MicroService in General"},{"location":"Projects/gempick/dev-materials/microservice%28k8s%29/microservice%28k8s%29/#microservice-design-patterns","text":"[Youtube] Microservices Architecture Design Patterns | 10 Design Principles | 26 Design Patterns","title":"MicroService Design Patterns"},{"location":"Projects/gempick/dev-materials/microservice%28k8s%29/microservice%28k8s%29/#circuit-breaker-pattern","text":"[Youtube] Circuit Breaker Pattern","title":"Circuit Breaker Pattern"},{"location":"Projects/gempick/dev-materials/microservice%28k8s%29/microservice%28k8s%29/#api-gateway","text":"","title":"API Gateway"},{"location":"Projects/gempick/dev-materials/microservice%28k8s%29/microservice%28k8s%29/#links","text":"[Youtube] API Gateway API Gateway pattern","title":"Links"},{"location":"Projects/gempick/dev-materials/microservice%28k8s%29/microservice%28k8s%29/#istio-service-mesh","text":"[Youtube] Istio Setup in Kubernetes | Step by Step Guide to install Istio Service Mesh [Youtube] Istio & Service Mesh - simply explained in 15 mins [Youtube] What is a service mesh? [Youtube] Istio Service","title":"Istio &amp; Service Mesh"},{"location":"Projects/gempick/dev-materials/microservice%28k8s%29/microservice%28k8s%29/#istio","text":"","title":"Istio"},{"location":"Projects/gempick/dev-materials/microservice%28k8s%29/microservice%28k8s%29/#challenge-of-microservice-archeitecture","text":"","title":"Challenge of MicroService Archeitecture"},{"location":"Projects/gempick/development-policy/API-documentation/","text":"","title":"API documentation"},{"location":"Projects/gempick/development-policy/aws/","text":"","title":"Aws"},{"location":"Projects/gempick/development-policy/git/","text":"","title":"Git"},{"location":"Projects/gempick/development-policy/microservice/","text":"","title":"Microservice"},{"location":"Projects/gempick/development-policy/source-code/","text":"","title":"Source code"},{"location":"Projects/gempick/etc/example/","text":"","title":"Example"},{"location":"Projects/gempick/general/example/","text":"","title":"Example"},{"location":"Projects/gempick/meeting/example/","text":"","title":"Example"},{"location":"Projects/gempick/microservice2/example/","text":"","title":"Example"},{"location":"Projects/gempick/microservice3/example/","text":"","title":"Example"},{"location":"Projects/gempick/user-auth/architecture/","text":"Architecture User-Auth microservice\uc5d0\ub294 \ud06c\uac8c \uc544\ub798\uc640 \uac19\uc774 3\uac00\uc9c0 \uc694\uc18c\ub97c \uac16\uace0 \uc788\ub2e4. OAuth2.0 and OpenID connect (with 3rd party providers) Token \uad00\ub9ac (Authentication & Authorization) User \uad00\ub9ac OAuth2.0 and OpenID Connect \ubcf8 \uc11c\ube44\uc2a4\uc5d0\uc11c\ub294 3rd party providers\ub97c \uc5f0\ub3d9\ud558\uc5ec login \uae30\ub2a5\uc744 \ub300\uccb4\ud558\uba70, Authorization server\uc5d0\uc11c \ubc1c\uae09 \ubc1b\uc740 token\uc744 \uc0ac\uc6a9\ud558\uc5ec resource server\uc5d0\uc11c \uc81c\uacf5\ud558\ub294 \uae30\ub2a5\uacfc scope\uc5d0 \ud3ec\ud568\ub41c \uc815\ubcf4\ub97c \uac00\uc838\uc628\ub2e4. \uc0ac\uc2e4\uc0c1 \ud504\ub85c\uc81d\ud2b8 \ucd08\uae30 \ubc84\uc83c\uc5d0\uc11c\ub294 Authorization server\uc640 \ud1b5\uc2e0\ud560 \ube48\ub3c4\uac00 \uc801\uc744 \uc608\uc815\uc774\uba70, \ub85c\uadf8\uc778 \uae30\ub2a5\uc744 \uc704\ud574 OAuth2.0 and OpenID connect\uc744 \uc0ac\uc6a9\ud558\uac8c \ub41c\ub2e4. \uc2e4\uc81c Client\uac00 Backend\uc640 \ud1b5\uc2e0\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9\ud558\ub294 token\uc740 User-Auth microservice\uc5d0\uc11c \ubc1c\uae09 \ubc0f \uad00\ub9ac \ud560 \uc608\uc815\uc774\ub2e4. OAuth2.0 Flow Features to implement 8. Authorization code\ub85c Access Token \uc694\uccad Client ID, Secret, Auth Code, etc\ub97c \ud3ec\ud568\ud558\uc5ec Authorization Server\uc5d0 Access token \uc694\uccad 9. Access Token \ubc1c\uae09 \ubc1c\uae09 \ubc1b\uc740 Access & Refresh Token\uc744 \uc548\uc804\ud558\uac8c \uc800\uc7a5 10. \uc778\uc99d \uc644\ub8cc \ubc0f \ub85c\uadf8\uc778 \uc131\uacf5 10\ubc88 \ub2e8\uacc4\uac00 \uc644\ub8cc \ub418\uba74 \ud544\uc694 \uc815\ubcf4\ub97c DB\uc5d0 \uc800\uc7a5\ud558\uace0 \ub0b4\ubd80\uc5d0\uc11c \uc0c8\ub85c\uc6b4 access token & refresh token\uc744 \ubc1c\uae09\ud558\uace0 client\uc5d0\uac8c \uc804\ub2ec\ud55c\ub2e4. 12, 13, 14\ub294 \uc544\uc9c1 \ubbf8\uc815 \uad6c\ud604 \ud544\uc694 API \uc815\uc758 /api/auth/.. /api/auth/.. /api/auth/.. Useful Links [Youtube] OAuth 2.0 and OpenID Connect OAuth2.0 \uac1c\ub150\uc815\ub9ac Access & Refresh Token (Authentication & Authorization) jwt\ud615\uc2dd\uc758 access, refresh token\uc744 user-auth ms\uc5d0\uc11c \uc9c1\uc811 \ubc1c\uae09\ud558\uba70 \uad00\ub9ac \ud55c\ub2e4. Token Sequence Diagram \ub85c\uadf8\uc778\uc744 \ud55c \uc0ac\uc6a9\uc790\uac00 Client\ub97c \ud1b5\ud574 API\ub97c \ud638\ucd9c \ud558\uc600\uc744\ub54c(Authentication & Authorization) Access token\uc774 \ub9cc\ub8cc\ub41c \uc0ac\uc6a9\uc790\uac00 \uc7ac\ubc1c\uae09\uc744 \uc694\uccad \ud558\uc600\uc744\ub54c \uad6c\ud604 \ud544\uc694 API \uc815\uc758 /api/auth/.. /api/auth/.. /api/auth/.. Authentication in MicroService Breif Idea API Gateway Service Mesh (Istio)","title":"Architecture"},{"location":"Projects/gempick/user-auth/architecture/#architecture","text":"User-Auth microservice\uc5d0\ub294 \ud06c\uac8c \uc544\ub798\uc640 \uac19\uc774 3\uac00\uc9c0 \uc694\uc18c\ub97c \uac16\uace0 \uc788\ub2e4. OAuth2.0 and OpenID connect (with 3rd party providers) Token \uad00\ub9ac (Authentication & Authorization) User \uad00\ub9ac","title":"Architecture"},{"location":"Projects/gempick/user-auth/architecture/#oauth20-and-openid-connect","text":"\ubcf8 \uc11c\ube44\uc2a4\uc5d0\uc11c\ub294 3rd party providers\ub97c \uc5f0\ub3d9\ud558\uc5ec login \uae30\ub2a5\uc744 \ub300\uccb4\ud558\uba70, Authorization server\uc5d0\uc11c \ubc1c\uae09 \ubc1b\uc740 token\uc744 \uc0ac\uc6a9\ud558\uc5ec resource server\uc5d0\uc11c \uc81c\uacf5\ud558\ub294 \uae30\ub2a5\uacfc scope\uc5d0 \ud3ec\ud568\ub41c \uc815\ubcf4\ub97c \uac00\uc838\uc628\ub2e4. \uc0ac\uc2e4\uc0c1 \ud504\ub85c\uc81d\ud2b8 \ucd08\uae30 \ubc84\uc83c\uc5d0\uc11c\ub294 Authorization server\uc640 \ud1b5\uc2e0\ud560 \ube48\ub3c4\uac00 \uc801\uc744 \uc608\uc815\uc774\uba70, \ub85c\uadf8\uc778 \uae30\ub2a5\uc744 \uc704\ud574 OAuth2.0 and OpenID connect\uc744 \uc0ac\uc6a9\ud558\uac8c \ub41c\ub2e4. \uc2e4\uc81c Client\uac00 Backend\uc640 \ud1b5\uc2e0\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9\ud558\ub294 token\uc740 User-Auth microservice\uc5d0\uc11c \ubc1c\uae09 \ubc0f \uad00\ub9ac \ud560 \uc608\uc815\uc774\ub2e4.","title":"OAuth2.0 and OpenID Connect"},{"location":"Projects/gempick/user-auth/architecture/#oauth20-flow","text":"","title":"OAuth2.0 Flow"},{"location":"Projects/gempick/user-auth/architecture/#features-to-implement","text":"","title":"Features to implement"},{"location":"Projects/gempick/user-auth/architecture/#8-authorization-code-access-token","text":"Client ID, Secret, Auth Code, etc\ub97c \ud3ec\ud568\ud558\uc5ec Authorization Server\uc5d0 Access token \uc694\uccad","title":"8. Authorization code\ub85c Access Token \uc694\uccad"},{"location":"Projects/gempick/user-auth/architecture/#9-access-token","text":"\ubc1c\uae09 \ubc1b\uc740 Access & Refresh Token\uc744 \uc548\uc804\ud558\uac8c \uc800\uc7a5","title":"9. Access Token \ubc1c\uae09"},{"location":"Projects/gempick/user-auth/architecture/#10","text":"10\ubc88 \ub2e8\uacc4\uac00 \uc644\ub8cc \ub418\uba74 \ud544\uc694 \uc815\ubcf4\ub97c DB\uc5d0 \uc800\uc7a5\ud558\uace0 \ub0b4\ubd80\uc5d0\uc11c \uc0c8\ub85c\uc6b4 access token & refresh token\uc744 \ubc1c\uae09\ud558\uace0 client\uc5d0\uac8c \uc804\ub2ec\ud55c\ub2e4.","title":"10. \uc778\uc99d \uc644\ub8cc \ubc0f \ub85c\uadf8\uc778 \uc131\uacf5"},{"location":"Projects/gempick/user-auth/architecture/#12-13-14","text":"","title":"12, 13, 14\ub294 \uc544\uc9c1 \ubbf8\uc815"},{"location":"Projects/gempick/user-auth/architecture/#api","text":"/api/auth/.. /api/auth/.. /api/auth/..","title":"\uad6c\ud604 \ud544\uc694 API \uc815\uc758"},{"location":"Projects/gempick/user-auth/architecture/#useful-links","text":"[Youtube] OAuth 2.0 and OpenID Connect OAuth2.0 \uac1c\ub150\uc815\ub9ac","title":"Useful Links"},{"location":"Projects/gempick/user-auth/architecture/#access-refresh-token-authentication-authorization","text":"jwt\ud615\uc2dd\uc758 access, refresh token\uc744 user-auth ms\uc5d0\uc11c \uc9c1\uc811 \ubc1c\uae09\ud558\uba70 \uad00\ub9ac \ud55c\ub2e4.","title":"Access &amp; Refresh Token (Authentication &amp; Authorization)"},{"location":"Projects/gempick/user-auth/architecture/#token-sequence-diagram","text":"\ub85c\uadf8\uc778\uc744 \ud55c \uc0ac\uc6a9\uc790\uac00 Client\ub97c \ud1b5\ud574 API\ub97c \ud638\ucd9c \ud558\uc600\uc744\ub54c(Authentication & Authorization) Access token\uc774 \ub9cc\ub8cc\ub41c \uc0ac\uc6a9\uc790\uac00 \uc7ac\ubc1c\uae09\uc744 \uc694\uccad \ud558\uc600\uc744\ub54c","title":"Token Sequence Diagram"},{"location":"Projects/gempick/user-auth/architecture/#api_1","text":"/api/auth/.. /api/auth/.. /api/auth/..","title":"\uad6c\ud604 \ud544\uc694 API \uc815\uc758"},{"location":"Projects/gempick/user-auth/architecture/#authentication-in-microservice","text":"","title":"Authentication in MicroService"},{"location":"Projects/gempick/user-auth/architecture/#breif-idea","text":"","title":"Breif Idea"},{"location":"Projects/gempick/user-auth/architecture/#api-gateway","text":"","title":"API Gateway"},{"location":"Projects/gempick/user-auth/architecture/#service-mesh-istio","text":"","title":"Service Mesh (Istio)"},{"location":"Projects/gempick/user-auth/database/","text":"Database","title":"Database"},{"location":"Projects/gempick/user-auth/database/#database","text":"","title":"Database"},{"location":"Projects/gempick/user-auth/etc/","text":"","title":"Etc"},{"location":"kubernetes/","text":"Kubernetes Notes Refer to a README file in each folder for further notes and example. Edit # edit a current running pod kubectl edit pod pod-name Get output # use --dry-run=client to preview the object only(doesnt submit) then write info into yaml file kubectl run redis --image = redis --dry-run = client -o yaml > pod-def.yml # write a current running pod info into yaml file kubectl get pod pod-name -o yaml > pod-def.yml Accessing a pod from another namespace Example URL db-service.dev.svc.cluster.local db-service : service name(deployment, pod, etc) dev : namespace svc : service cluster.local : domain Labels and Selector You can use the labels for retrieving and filtering the data from the Kubernetes API. Assign labels to pod and search # To assgin label k label pod <pod-name> <label-key> = <label-val> <label-key2> = <label-val2> # to overwrite k label pod <pod-name> <label-key> = <label-val> --overwrite # To check pod labels kubectl get pod --show-labels # or search pod with label kubectl get pod -L <key-name> kubectl get pod -L <key-name> = <val-name> k get pod --selector <key-name> = <val-name> # delete specific pod with label k delete pod --selector <key-name> = <val-name> # to delete label from pod k label pod <pod-name> <label-kery>- Label Definition at each level Selector labels and Pod labels must be the same apiVersion : apps/v1 kind : Deployment metadata : name : my-nginx labels : app : dep-label # Deployment labels <--this label is to manage the deployment itself. this may be used to filter the deployment based on this label. Check dep label by k get deploy <dep-name> --show-labels spec : replicas : 2 selector : matchLabels : app : my-nginx # Selector labels: <-- this is where we tell replication controller to manage the pods. This field defines how the Deployment finds which Pods to manage. Must match with Pod labels template : metadata : labels : app : my-nginx # Pod labels: <--this is the label of the pod, this must be same as Selector labels. Check pod label by k get pod <pod-name> --show-labels spec : containers : - name : my-nginx image : nginx:alpine ports : - containerPort : 80 resources : limits : memory : \"128Mi\" #128 MB cpu : \"200m\" #200 millicpu (.2 cpu or 20% of the cpu) Anotation Annotations are the second way of attaching metadata to the Kubernetes resources. Annotations are NOT used to identify and select objects Labels are for Kubernetes, while annotations are for humans. Anotation Example apiVersion : v1 kind : Pod metadata : name : nginx-pod annotations : builder : sungkim buildDate : 1111111 imageRegistry : https://hub.docker.com spec : containers : - name : nginx image : nginx ports : - containerPort : 8080 # To check annotation k describe pod <pod-name> Scale # After creatuib of deployment, use scale command. This doesn't change the corresponding yaml file. kubectl scale deployment nginx --replicas = 4 Service Imperative command example: Create a service name redis-sevice to expose pod redis # assume pod exists already. this maps service to pod redis kubectl expose pod redis --port = 6379 --name redis-service --dry-run = client -o yaml Create a pod and service at the same time and expose service to pod Example: # pod and service creatation at the same time # create a pod and service called httpd and expose service to the pod at once. # and the target port for the service is 80 $ kubectl run httpd --image = httpd:alpine --port 80 --expose --dry-run = client -o yaml Corresponding YAML file: apiVersion : v1 kind : Service metadata : name : httpd spec : ports : - port : 80 protocol : TCP targetPort : 80 selector : run : httpd # --expose option ensures the selector matches the labels from pod(**) --- --- apiVersion : v1 kind : Pod metadata : labels : run : httpd # this part ** name : httpd spec : containers : - image : httpd:alpine name : httpd ports : - containerPort : 80 Log To view the entire logs of replicated pods k logs -l <label-key> = <val> k logs -l app = webui -f","title":"Kubernetes Notes"},{"location":"kubernetes/#kubernetes-notes","text":"Refer to a README file in each folder for further notes and example.","title":"Kubernetes Notes"},{"location":"kubernetes/#edit","text":"# edit a current running pod kubectl edit pod pod-name","title":"Edit"},{"location":"kubernetes/#get-output","text":"# use --dry-run=client to preview the object only(doesnt submit) then write info into yaml file kubectl run redis --image = redis --dry-run = client -o yaml > pod-def.yml # write a current running pod info into yaml file kubectl get pod pod-name -o yaml > pod-def.yml","title":"Get output"},{"location":"kubernetes/#accessing-a-pod-from-another-namespace","text":"","title":"Accessing a pod from another namespace"},{"location":"kubernetes/#example-url","text":"db-service.dev.svc.cluster.local db-service : service name(deployment, pod, etc) dev : namespace svc : service cluster.local : domain","title":"Example URL"},{"location":"kubernetes/#labels-and-selector","text":"You can use the labels for retrieving and filtering the data from the Kubernetes API.","title":"Labels and Selector"},{"location":"kubernetes/#assign-labels-to-pod-and-search","text":"# To assgin label k label pod <pod-name> <label-key> = <label-val> <label-key2> = <label-val2> # to overwrite k label pod <pod-name> <label-key> = <label-val> --overwrite # To check pod labels kubectl get pod --show-labels # or search pod with label kubectl get pod -L <key-name> kubectl get pod -L <key-name> = <val-name> k get pod --selector <key-name> = <val-name> # delete specific pod with label k delete pod --selector <key-name> = <val-name> # to delete label from pod k label pod <pod-name> <label-kery>-","title":"Assign labels to pod and search"},{"location":"kubernetes/#label-definition-at-each-level","text":"Selector labels and Pod labels must be the same apiVersion : apps/v1 kind : Deployment metadata : name : my-nginx labels : app : dep-label # Deployment labels <--this label is to manage the deployment itself. this may be used to filter the deployment based on this label. Check dep label by k get deploy <dep-name> --show-labels spec : replicas : 2 selector : matchLabels : app : my-nginx # Selector labels: <-- this is where we tell replication controller to manage the pods. This field defines how the Deployment finds which Pods to manage. Must match with Pod labels template : metadata : labels : app : my-nginx # Pod labels: <--this is the label of the pod, this must be same as Selector labels. Check pod label by k get pod <pod-name> --show-labels spec : containers : - name : my-nginx image : nginx:alpine ports : - containerPort : 80 resources : limits : memory : \"128Mi\" #128 MB cpu : \"200m\" #200 millicpu (.2 cpu or 20% of the cpu)","title":"Label Definition at each level"},{"location":"kubernetes/#anotation","text":"Annotations are the second way of attaching metadata to the Kubernetes resources. Annotations are NOT used to identify and select objects Labels are for Kubernetes, while annotations are for humans.","title":"Anotation"},{"location":"kubernetes/#anotation-example","text":"apiVersion : v1 kind : Pod metadata : name : nginx-pod annotations : builder : sungkim buildDate : 1111111 imageRegistry : https://hub.docker.com spec : containers : - name : nginx image : nginx ports : - containerPort : 8080 # To check annotation k describe pod <pod-name>","title":"Anotation Example"},{"location":"kubernetes/#scale","text":"# After creatuib of deployment, use scale command. This doesn't change the corresponding yaml file. kubectl scale deployment nginx --replicas = 4","title":"Scale"},{"location":"kubernetes/#service","text":"Imperative command example:","title":"Service"},{"location":"kubernetes/#create-a-service-name-redis-sevice-to-expose-pod-redis","text":"# assume pod exists already. this maps service to pod redis kubectl expose pod redis --port = 6379 --name redis-service --dry-run = client -o yaml","title":"Create a service name redis-sevice to expose pod redis"},{"location":"kubernetes/#create-a-pod-and-service-at-the-same-time-and-expose-service-to-pod","text":"Example: # pod and service creatation at the same time # create a pod and service called httpd and expose service to the pod at once. # and the target port for the service is 80 $ kubectl run httpd --image = httpd:alpine --port 80 --expose --dry-run = client -o yaml Corresponding YAML file: apiVersion : v1 kind : Service metadata : name : httpd spec : ports : - port : 80 protocol : TCP targetPort : 80 selector : run : httpd # --expose option ensures the selector matches the labels from pod(**) --- --- apiVersion : v1 kind : Pod metadata : labels : run : httpd # this part ** name : httpd spec : containers : - image : httpd:alpine name : httpd ports : - containerPort : 80","title":"Create a pod and service at the same time and expose service to pod"},{"location":"kubernetes/#log","text":"To view the entire logs of replicated pods k logs -l <label-key> = <val> k logs -l app = webui -f","title":"Log"},{"location":"kubernetes/controller/","text":"Controller Controller ensures that the specified number of pod replicas are running at any point of time. Replication Controller ReplicaSet Deployment(Rolling-Update & Rollback) DaemonSet StatefulSet Job CronJob 1. Replication Controller ReplicationController YAML Example apiVersion : v1 kind : ReplicationController metadata : name : nginx-rep spec : replicas : 3 selector : app : webui # must match with one of pod labels version : \"2.1\" # can be multiple, if multiple labels then selector uses \"AND\" condition template : # pod template metadata : name : nginx-pod labels : # pod labels app : webui version : \"2.1\" # can have multiple lables spec : containers : - name : nginx-container image : nginx Edit number of scales kubectl edit rc <rc-name> # or kubectl scale rc <rc-name> --replicas = N Delete Replication Controller # this will delete the entire running pod.. kubectl delete rc <rc-name> # --cascade=false will keep pods alive kubectl delete rc <rc-name> --cascade = false 2. ReplicaSet Almost identical to replication controller, but provides diverse selector options. ReplicaSet YAML Example apiVersion : apps/v1 kind : ReplicaSet metadata : name : nginx-rep spec : replicas : 3 selector : matchLabels : app : webui matchExpressions : - { key : version , operator : In , values : [ \"2.1\" , \"2.2\" ]} # - {key: version, operator: Exists } template : # pod template metadata : name : nginx-pod labels : # pod labels app : webui version : \"2.1\" # can have multiple lables spec : containers : - name : nginx-container image : nginx Operator type: * In * NotIn * Exists: if key matches, include every values * DoesNotExist 3. Deployment Deployment controls ReplicaSet Deployment -> ReplicaSet -> Pod Deployment is exactly same as ReplicaSet unless you define rollingupdate . Deployment YAML Example apiVersion : apps/v1 kind : Deployment metadata : name : nginx-dep spec : replicas : 3 selector : matchLabels : app : webui template : # pod template metadata : name : nginx-pod labels : # pod labels app : webui version : \"2.1\" # can have multiple lables spec : containers : - name : nginx-container image : nginx How Deployment works example Deployment -> ReplicaSet -> Pod $ kubectl get deploy,rs,pod NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-dep 3 /3 3 3 30s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-dep-546b4b7cbc 3 3 3 30s NAME READY STATUS RESTARTS AGE pod/nginx-dep-546b4b7cbc-d29qt 1 /1 Running 0 30s pod/nginx-dep-546b4b7cbc-f7sw9 1 /1 Running 0 30s pod/nginx-dep-546b4b7cbc-q75cg 1 /1 Running 0 30s Delete Deployment kubectl delete deployments <dp-name> # or kubectl delete -f path/to/dep.yml Delete ReplicaSet If ReplicaSet is deleted, then Deployment will re-schedule ReplicaSet again, then ReplicaSet will create pod . Rolling update & Rolling Back(Deployment) There are 3 ways to apply rolling-update . 1. set command 2. YAML apply 3. edit command Rolling update 1. Rolling-update via set command set command help you make changes to existing application resources. Run the following command to update deployment while deployment is already up and run. kubectl set image deployment <deploy-name> <container-name> = <new-ver-image> --record 2. Rolling-update via YAML file Modify the image version and annotations , then kubectl apply to perform rolling-update. Applying the modified yaml file is nicer way for rolling-update because it keeps track of version information on yaml file. kind : Deployment metadata : annotations : kubernetes.io/change-cause : version 1.16 # change this ... spec : containers : - name : web image : nginx:1.16 # change image version The value of kubernetes.io/change-cause key gets tracked on rollout history . Apply rolling-update via YAML after modifying YAML file. $ kubectl apply -f path/to/deploy.yml $ kubectl rollout history deployment <dep-name> # ex) deployment.apps/nginx-dep REVISION CHANGE-CAUSE 1 version 1 .15 2 version 1 .16 3. Rolling-update via edit command $ kubectl edit deployment <dep-name> --record # history check ex) $ kubectl rollout history deploy nginx-dep deployment.apps/nginx-dep REVISION CHANGE-CAUSE 1 version 1 .15 2 kubectl edit deployment nginx-dep --record = true Rolling-update status check Check status of rolling-update while rolling-update is being executed. kubectl rollout status deployment <deploy-name> Rolling-update pause/resume Pause/resume rolling-update while rolling-update is being executed. kubectl rollout pause deployment <deploy-name> # resume kubectl rollout resume deployment <deploy-name> Rollout history check kubectl rollout history deployment <deploy-name> # ex) deployment.apps/nginx-dep REVISION CHANGE-CAUSE 1 kubectl create --filename = controller/deployment.yml --record = true 2 kubectl set image deployment nginx-dep web = nginx:1.15 --record = true 3 kubectl set image deployment nginx-dep web = nginx:1.16 --record = true 4 kubectl set image deployment nginx-dep web = nginx:1.17 --record = true How to control Rolling-update spec : progressDeadlineSeconds : 600 replicas : 3 revisionHistoryLimit : 10 selector : matchLabels : app : webui strategy : rollingUpdate : maxSurge : 25% maxUnavailable : 25% type : RollingUpdate progressDeadlineSeconds : rollback the update if update takes too long. revisionHistoryLimit : the number of old ReplicaSets to retain to allow rollback. maxSurge : the maximum number of Pods that can be created over the desired number of Pods. if replicas=3 and maxSurge=50% , 3 x 50% = 2(round up). 2 + 3(replicas) = 5. Thus, 2 additional pods gets created during rolling-update. maxUnavailable : the maximum number of Pods that can be unavailable during the update process. if replicas=3 and maxUnavailable=50% , then 2 pods get terminated during rolling-update. Rollback # rollback to the previous version kubectl rollout undo deploy <deploy-name> # select the version to rollback(use the index from history) kubectl rollout undo deploy <deploy-name> --to-revision = N 4. DaemonSet DaemonSet ensures that a particular pod gets placed every node(one pod at a node). The pod gets automatically scheduled/deleted as a new Node gets createed/deleted. DaemonSet Example IMPORTANT: No replicas attribute apiVersion : apps/v1 kind : DaemonSet metadata : name : nginx-daemonset spec : selector : matchLabels : app : webui template : # pod template metadata : name : nginx-pod labels : # pod labels app : webui spec : containers : - name : web image : nginx:1.15 DaemonSet rolling-update via apply command Edit YAML file(image version), then use apply command $ kubectl apply -f controller/daemonset.yml IMPORTANT DaemonSet rolling-update behaves a bit different from deployment rolling-update. it doesn't create a new pod then terminate, but instead it terminates the pod then create a new pod. rollout undo and checking history work in the same manner as Deployment . DaemonSet delete kubectl delete ds <daemonset-name> 5. StatefulSet Manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods. If you want to use storage volumes to provide persistence for your workload, you can use a StatefulSet as part of the solution. statefulSet doesn't ensure a pod to be placed at each node like daemonset , but it ensures the order and name of pod. statefulSet example YAML apiVersion : apps/v1 kind : StatefulSet metadata : name : nginx-stateful spec : replicas : 3 serviceName : sf-service podManagementPolicy : Parallel # OrderedReady # optional selector : matchLabels : app : webui template : # pod template metadata : name : nginx-pod labels : # pod labels app : webui spec : containers : - name : web image : nginx:1.15 Pod's name example $ kubectl get pod nginx-stateful-0 1 /1 Running 0 4m58s nginx-stateful-1 1 /1 Running 0 4m57s nginx-stateful-2 1 /1 Running 0 4m56s If you delete nginx-stateful-1, then a pod with the exact same name gets scheduled again, but it doesn't guarantee to place the pod on the same node. scale up & down Check how pod's name changes when scale goes up/down. kubectl scale statefulset <sf-name> --replicas = N ## ex) from scale 3 to 5 NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-stateful-0 1 /1 Running 0 78s 10 .44.0.1 worker1 <none> <none> nginx-stateful-1 1 /1 Running 0 77s 10 .36.0.4 worker2 <none> <none> nginx-stateful-2 1 /1 Running 0 75s 10 .44.0.2 worker1 <none> <none> nginx-stateful-3 1 /1 Running 0 12s 10 .36.0.5 worker2 <none> <none> nginx-stateful-4 1 /1 Running 0 10s 10 .44.0.3 worker1 <none> <none> # ex) from scale 5 to 2 NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-stateful-0 1 /1 Running 0 117s 10 .44.0.1 worker1 <none> <none> nginx-stateful-1 1 /1 Running 0 116s 10 .36.0.4 worker2 <none> <none> StatefulSet Rolling-update Rolling-update terminates the pod then create a new pod like DaemonSet . It guarantees the pods to be placed on the same node with version changes. 6. Job Controller Job Controller are primarily used for batch processing. In a nutshell, * Job(task) completed -> Pod in completed status, but alive * Job(task) fails -> Pod gets recreated the specified time. Pods can be either terminated or alive depending on options on restartPolicy Check pod's lifecyle and how restartPolicy works in pod. restartPolicy manages container, not pod. When a pod is failed An entire Pod can also fail. ex) when the pod is kicked off the node (node is upgraded, rebooted, deleted, etc.), or if a container of the Pod fails and the .spec.template.spec.restartPolicy = \"Never\" . When a Pod fails, then the Job controller starts a new Pod. This means that your application needs to handle the case when it is restarted in a new pod. backoffLimit policy You can fail a Job after some amount of retries. Set .spec.backoffLimit to specify the number of retries(pod recreation) before considering a Job as failed. Default is .spec.backoffLimit=6 . Job Controller YAML example apiVersion : batch/v1 kind : Job metadata : name : nginx-job spec : # completions: 5 # run N time # parallelism: 2 activeDeadlineSeconds : 5 # pod terminated after activeDeadlineSeconds template : # pod template spec : containers : - name : centos-container image : centos:7 command : [ \"bash\" ] args : - \"-c\" - \"echo 'Hello'; sleep 10; echo 'bye'\" restartPolicy : Never backoffLimit : 2 When a task completed Even if a task completed without a problem, the pod is still alive. restartPolicy: Always is invalid value on Job. Output example NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-job-skmg2 0 /1 Completed 0 5m33s 10 .44.0.1 worker1 <none> <none> When a task fails and restartPolicy: Never (Recommanded) restartPolicy: Never will not restart container even if it fails, thus the job(task) stays as failed, and 'Job Controller` will restart(create a new pod) the pod until the job backoff limit has been reached. Pods are still alive. spec : template : spec : ... restartPolicy : Never backoffLimit : 2 Result $ kubectl get pod <pod-name> # example NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-job-8xfmt 0 /1 StartError 0 4s 10 .44.0.1 worker1 <none> <none> nginx-job-z6kdp 0 /1 StartError 0 17s 10 .44.0.1 worker1 <none> <none> nginx-job-z6pj4 0 /1 StartError 0 20s 10 .44.0.1 worker1 <none> <none> When a task fails and restartPolicy: OnFailure restartPolicy: OnFailure restarts container when fails. Once the job backoff limit has been reached, your Pod running the Job will be terminated(pod dies) . This makes debugging the job's executable mroe difficult. spec : template : spec : ... restartPolicy : OnFailure backoffLimit : 2 Result $ kubectl get pod <pod-name> # example NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES 7. CronJob A CronJob creates Jobs on a repeating schedule just like crontab on Linux. IMPORTANT All CronJob schedule: times are based on the timezone of the kube-controller-manager . example.yaml apiVersion : batch/v1 kind : CronJob metadata : name : nginx-cronjob spec : schedule : \"* * * * *\" concurrencyPolicy : Allow # Forbid # allow cronjob to run concurrently if needed startDeadlineSeconds : 500 successfulJobsHistoryLimit : 3 # only keep 3 succesful pod history run by cronjob when kubctl get pod <pod-name> jobTemplate : spec : template : spec : containers : - name : centos-container image : centos:7 command : [ \"bash\" ] args : - \"-c\" - \"echo 'Hello'; sleep 10; echo 'bye'\" restartPolicy : Never backoffLimit : 3 Cron schedule syntax # \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59) # \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23) # \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the month (1 - 31) # \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1 - 12) # \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the week (0 - 6) (Sunday to Saturday; # \u2502 \u2502 \u2502 \u2502 \u2502 7 is also Sunday on some systems) # \u2502 \u2502 \u2502 \u2502 \u2502 OR sun, mon, tue, wed, thu, fri, sat # \u2502 \u2502 \u2502 \u2502 \u2502 # * * * * * Example every weekday at 3:00 am 0 3 * 1-5 every weekend at 3:00 am 0 3 * 0,6 every 5 minute /5 * * *","title":"controller"},{"location":"kubernetes/controller/#controller","text":"Controller ensures that the specified number of pod replicas are running at any point of time. Replication Controller ReplicaSet Deployment(Rolling-Update & Rollback) DaemonSet StatefulSet Job CronJob","title":"Controller"},{"location":"kubernetes/controller/#1-replication-controller","text":"","title":"1. Replication Controller"},{"location":"kubernetes/controller/#replicationcontroller-yaml-example","text":"apiVersion : v1 kind : ReplicationController metadata : name : nginx-rep spec : replicas : 3 selector : app : webui # must match with one of pod labels version : \"2.1\" # can be multiple, if multiple labels then selector uses \"AND\" condition template : # pod template metadata : name : nginx-pod labels : # pod labels app : webui version : \"2.1\" # can have multiple lables spec : containers : - name : nginx-container image : nginx","title":"ReplicationController YAML Example"},{"location":"kubernetes/controller/#edit-number-of-scales","text":"kubectl edit rc <rc-name> # or kubectl scale rc <rc-name> --replicas = N","title":"Edit number of scales"},{"location":"kubernetes/controller/#delete-replication-controller","text":"# this will delete the entire running pod.. kubectl delete rc <rc-name> # --cascade=false will keep pods alive kubectl delete rc <rc-name> --cascade = false","title":"Delete Replication Controller"},{"location":"kubernetes/controller/#2-replicaset","text":"Almost identical to replication controller, but provides diverse selector options.","title":"2. ReplicaSet"},{"location":"kubernetes/controller/#replicaset-yaml-example","text":"apiVersion : apps/v1 kind : ReplicaSet metadata : name : nginx-rep spec : replicas : 3 selector : matchLabels : app : webui matchExpressions : - { key : version , operator : In , values : [ \"2.1\" , \"2.2\" ]} # - {key: version, operator: Exists } template : # pod template metadata : name : nginx-pod labels : # pod labels app : webui version : \"2.1\" # can have multiple lables spec : containers : - name : nginx-container image : nginx Operator type: * In * NotIn * Exists: if key matches, include every values * DoesNotExist","title":"ReplicaSet YAML Example"},{"location":"kubernetes/controller/#3-deployment","text":"Deployment controls ReplicaSet Deployment -> ReplicaSet -> Pod Deployment is exactly same as ReplicaSet unless you define rollingupdate .","title":"3. Deployment"},{"location":"kubernetes/controller/#deployment-yaml-example","text":"apiVersion : apps/v1 kind : Deployment metadata : name : nginx-dep spec : replicas : 3 selector : matchLabels : app : webui template : # pod template metadata : name : nginx-pod labels : # pod labels app : webui version : \"2.1\" # can have multiple lables spec : containers : - name : nginx-container image : nginx","title":"Deployment YAML Example"},{"location":"kubernetes/controller/#how-deployment-works-example","text":"Deployment -> ReplicaSet -> Pod $ kubectl get deploy,rs,pod NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-dep 3 /3 3 3 30s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-dep-546b4b7cbc 3 3 3 30s NAME READY STATUS RESTARTS AGE pod/nginx-dep-546b4b7cbc-d29qt 1 /1 Running 0 30s pod/nginx-dep-546b4b7cbc-f7sw9 1 /1 Running 0 30s pod/nginx-dep-546b4b7cbc-q75cg 1 /1 Running 0 30s","title":"How Deployment works example"},{"location":"kubernetes/controller/#delete-deployment","text":"kubectl delete deployments <dp-name> # or kubectl delete -f path/to/dep.yml","title":"Delete Deployment"},{"location":"kubernetes/controller/#delete-replicaset","text":"If ReplicaSet is deleted, then Deployment will re-schedule ReplicaSet again, then ReplicaSet will create pod .","title":"Delete ReplicaSet"},{"location":"kubernetes/controller/#rolling-update-rolling-backdeployment","text":"There are 3 ways to apply rolling-update . 1. set command 2. YAML apply 3. edit command","title":"Rolling update &amp; Rolling Back(Deployment)"},{"location":"kubernetes/controller/#rolling-update","text":"","title":"Rolling update"},{"location":"kubernetes/controller/#1-rolling-update-via-set-command","text":"set command help you make changes to existing application resources. Run the following command to update deployment while deployment is already up and run. kubectl set image deployment <deploy-name> <container-name> = <new-ver-image> --record","title":"1. Rolling-update via set command"},{"location":"kubernetes/controller/#2-rolling-update-via-yaml-file","text":"Modify the image version and annotations , then kubectl apply to perform rolling-update. Applying the modified yaml file is nicer way for rolling-update because it keeps track of version information on yaml file. kind : Deployment metadata : annotations : kubernetes.io/change-cause : version 1.16 # change this ... spec : containers : - name : web image : nginx:1.16 # change image version The value of kubernetes.io/change-cause key gets tracked on rollout history . Apply rolling-update via YAML after modifying YAML file. $ kubectl apply -f path/to/deploy.yml $ kubectl rollout history deployment <dep-name> # ex) deployment.apps/nginx-dep REVISION CHANGE-CAUSE 1 version 1 .15 2 version 1 .16","title":"2. Rolling-update via YAML file"},{"location":"kubernetes/controller/#3-rolling-update-via-edit-command","text":"$ kubectl edit deployment <dep-name> --record # history check ex) $ kubectl rollout history deploy nginx-dep deployment.apps/nginx-dep REVISION CHANGE-CAUSE 1 version 1 .15 2 kubectl edit deployment nginx-dep --record = true","title":"3. Rolling-update via edit command"},{"location":"kubernetes/controller/#rolling-update-status-check","text":"Check status of rolling-update while rolling-update is being executed. kubectl rollout status deployment <deploy-name>","title":"Rolling-update status check"},{"location":"kubernetes/controller/#rolling-update-pauseresume","text":"Pause/resume rolling-update while rolling-update is being executed. kubectl rollout pause deployment <deploy-name> # resume kubectl rollout resume deployment <deploy-name> Rollout history check kubectl rollout history deployment <deploy-name> # ex) deployment.apps/nginx-dep REVISION CHANGE-CAUSE 1 kubectl create --filename = controller/deployment.yml --record = true 2 kubectl set image deployment nginx-dep web = nginx:1.15 --record = true 3 kubectl set image deployment nginx-dep web = nginx:1.16 --record = true 4 kubectl set image deployment nginx-dep web = nginx:1.17 --record = true","title":"Rolling-update pause/resume"},{"location":"kubernetes/controller/#how-to-control-rolling-update","text":"spec : progressDeadlineSeconds : 600 replicas : 3 revisionHistoryLimit : 10 selector : matchLabels : app : webui strategy : rollingUpdate : maxSurge : 25% maxUnavailable : 25% type : RollingUpdate progressDeadlineSeconds : rollback the update if update takes too long. revisionHistoryLimit : the number of old ReplicaSets to retain to allow rollback. maxSurge : the maximum number of Pods that can be created over the desired number of Pods. if replicas=3 and maxSurge=50% , 3 x 50% = 2(round up). 2 + 3(replicas) = 5. Thus, 2 additional pods gets created during rolling-update. maxUnavailable : the maximum number of Pods that can be unavailable during the update process. if replicas=3 and maxUnavailable=50% , then 2 pods get terminated during rolling-update.","title":"How to control Rolling-update"},{"location":"kubernetes/controller/#rollback","text":"# rollback to the previous version kubectl rollout undo deploy <deploy-name> # select the version to rollback(use the index from history) kubectl rollout undo deploy <deploy-name> --to-revision = N","title":"Rollback"},{"location":"kubernetes/controller/#4-daemonset","text":"DaemonSet ensures that a particular pod gets placed every node(one pod at a node). The pod gets automatically scheduled/deleted as a new Node gets createed/deleted.","title":"4. DaemonSet"},{"location":"kubernetes/controller/#daemonset-example","text":"IMPORTANT: No replicas attribute apiVersion : apps/v1 kind : DaemonSet metadata : name : nginx-daemonset spec : selector : matchLabels : app : webui template : # pod template metadata : name : nginx-pod labels : # pod labels app : webui spec : containers : - name : web image : nginx:1.15","title":"DaemonSet Example"},{"location":"kubernetes/controller/#daemonset-rolling-update-via-apply-command","text":"Edit YAML file(image version), then use apply command $ kubectl apply -f controller/daemonset.yml IMPORTANT DaemonSet rolling-update behaves a bit different from deployment rolling-update. it doesn't create a new pod then terminate, but instead it terminates the pod then create a new pod. rollout undo and checking history work in the same manner as Deployment .","title":"DaemonSet rolling-update via apply command"},{"location":"kubernetes/controller/#daemonset-delete","text":"kubectl delete ds <daemonset-name>","title":"DaemonSet delete"},{"location":"kubernetes/controller/#5-statefulset","text":"Manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods. If you want to use storage volumes to provide persistence for your workload, you can use a StatefulSet as part of the solution. statefulSet doesn't ensure a pod to be placed at each node like daemonset , but it ensures the order and name of pod.","title":"5. StatefulSet"},{"location":"kubernetes/controller/#statefulset-example-yaml","text":"apiVersion : apps/v1 kind : StatefulSet metadata : name : nginx-stateful spec : replicas : 3 serviceName : sf-service podManagementPolicy : Parallel # OrderedReady # optional selector : matchLabels : app : webui template : # pod template metadata : name : nginx-pod labels : # pod labels app : webui spec : containers : - name : web image : nginx:1.15","title":"statefulSet example YAML"},{"location":"kubernetes/controller/#pods-name-example","text":"$ kubectl get pod nginx-stateful-0 1 /1 Running 0 4m58s nginx-stateful-1 1 /1 Running 0 4m57s nginx-stateful-2 1 /1 Running 0 4m56s If you delete nginx-stateful-1, then a pod with the exact same name gets scheduled again, but it doesn't guarantee to place the pod on the same node.","title":"Pod's name example"},{"location":"kubernetes/controller/#scale-up-down","text":"Check how pod's name changes when scale goes up/down. kubectl scale statefulset <sf-name> --replicas = N ## ex) from scale 3 to 5 NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-stateful-0 1 /1 Running 0 78s 10 .44.0.1 worker1 <none> <none> nginx-stateful-1 1 /1 Running 0 77s 10 .36.0.4 worker2 <none> <none> nginx-stateful-2 1 /1 Running 0 75s 10 .44.0.2 worker1 <none> <none> nginx-stateful-3 1 /1 Running 0 12s 10 .36.0.5 worker2 <none> <none> nginx-stateful-4 1 /1 Running 0 10s 10 .44.0.3 worker1 <none> <none> # ex) from scale 5 to 2 NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-stateful-0 1 /1 Running 0 117s 10 .44.0.1 worker1 <none> <none> nginx-stateful-1 1 /1 Running 0 116s 10 .36.0.4 worker2 <none> <none>","title":"scale up &amp; down"},{"location":"kubernetes/controller/#statefulset-rolling-update","text":"Rolling-update terminates the pod then create a new pod like DaemonSet . It guarantees the pods to be placed on the same node with version changes.","title":"StatefulSet Rolling-update"},{"location":"kubernetes/controller/#6-job-controller","text":"Job Controller are primarily used for batch processing. In a nutshell, * Job(task) completed -> Pod in completed status, but alive * Job(task) fails -> Pod gets recreated the specified time. Pods can be either terminated or alive depending on options on restartPolicy Check pod's lifecyle and how restartPolicy works in pod. restartPolicy manages container, not pod.","title":"6. Job Controller"},{"location":"kubernetes/controller/#when-a-pod-is-failed","text":"An entire Pod can also fail. ex) when the pod is kicked off the node (node is upgraded, rebooted, deleted, etc.), or if a container of the Pod fails and the .spec.template.spec.restartPolicy = \"Never\" . When a Pod fails, then the Job controller starts a new Pod. This means that your application needs to handle the case when it is restarted in a new pod.","title":"When a pod is failed"},{"location":"kubernetes/controller/#backofflimit-policy","text":"You can fail a Job after some amount of retries. Set .spec.backoffLimit to specify the number of retries(pod recreation) before considering a Job as failed. Default is .spec.backoffLimit=6 .","title":"backoffLimit policy"},{"location":"kubernetes/controller/#job-controller-yaml-example","text":"apiVersion : batch/v1 kind : Job metadata : name : nginx-job spec : # completions: 5 # run N time # parallelism: 2 activeDeadlineSeconds : 5 # pod terminated after activeDeadlineSeconds template : # pod template spec : containers : - name : centos-container image : centos:7 command : [ \"bash\" ] args : - \"-c\" - \"echo 'Hello'; sleep 10; echo 'bye'\" restartPolicy : Never backoffLimit : 2","title":"Job Controller YAML example"},{"location":"kubernetes/controller/#when-a-task-completed","text":"Even if a task completed without a problem, the pod is still alive. restartPolicy: Always is invalid value on Job. Output example NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-job-skmg2 0 /1 Completed 0 5m33s 10 .44.0.1 worker1 <none> <none>","title":"When a task completed"},{"location":"kubernetes/controller/#when-a-task-fails-and-restartpolicy-never-recommanded","text":"restartPolicy: Never will not restart container even if it fails, thus the job(task) stays as failed, and 'Job Controller` will restart(create a new pod) the pod until the job backoff limit has been reached. Pods are still alive. spec : template : spec : ... restartPolicy : Never backoffLimit : 2 Result $ kubectl get pod <pod-name> # example NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-job-8xfmt 0 /1 StartError 0 4s 10 .44.0.1 worker1 <none> <none> nginx-job-z6kdp 0 /1 StartError 0 17s 10 .44.0.1 worker1 <none> <none> nginx-job-z6pj4 0 /1 StartError 0 20s 10 .44.0.1 worker1 <none> <none>","title":"When a task fails and restartPolicy: Never (Recommanded)"},{"location":"kubernetes/controller/#when-a-task-fails-and-restartpolicy-onfailure","text":"restartPolicy: OnFailure restarts container when fails. Once the job backoff limit has been reached, your Pod running the Job will be terminated(pod dies) . This makes debugging the job's executable mroe difficult. spec : template : spec : ... restartPolicy : OnFailure backoffLimit : 2 Result $ kubectl get pod <pod-name> # example NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES","title":"When a task fails and restartPolicy: OnFailure"},{"location":"kubernetes/controller/#7-cronjob","text":"A CronJob creates Jobs on a repeating schedule just like crontab on Linux. IMPORTANT All CronJob schedule: times are based on the timezone of the kube-controller-manager . example.yaml apiVersion : batch/v1 kind : CronJob metadata : name : nginx-cronjob spec : schedule : \"* * * * *\" concurrencyPolicy : Allow # Forbid # allow cronjob to run concurrently if needed startDeadlineSeconds : 500 successfulJobsHistoryLimit : 3 # only keep 3 succesful pod history run by cronjob when kubctl get pod <pod-name> jobTemplate : spec : template : spec : containers : - name : centos-container image : centos:7 command : [ \"bash\" ] args : - \"-c\" - \"echo 'Hello'; sleep 10; echo 'bye'\" restartPolicy : Never backoffLimit : 3","title":"7. CronJob"},{"location":"kubernetes/controller/#cron-schedule-syntax","text":"# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59) # \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23) # \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the month (1 - 31) # \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 month (1 - 12) # \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 day of the week (0 - 6) (Sunday to Saturday; # \u2502 \u2502 \u2502 \u2502 \u2502 7 is also Sunday on some systems) # \u2502 \u2502 \u2502 \u2502 \u2502 OR sun, mon, tue, wed, thu, fri, sat # \u2502 \u2502 \u2502 \u2502 \u2502 # * * * * * Example every weekday at 3:00 am 0 3 * 1-5 every weekend at 3:00 am 0 3 * 0,6 every 5 minute /5 * * *","title":"Cron schedule syntax"}]}